{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1192499,"sourceType":"datasetVersion","datasetId":622510}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-02T14:58:41.600681Z","iopub.execute_input":"2024-07-02T14:58:41.601346Z","iopub.status.idle":"2024-07-02T14:58:41.979733Z","shell.execute_reply.started":"2024-07-02T14:58:41.601290Z","shell.execute_reply":"2024-07-02T14:58:41.978813Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/sentiment-analysis-for-financial-news/all-data.csv\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/Sentences_66Agree.txt\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/Sentences_AllAgree.txt\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/README.txt\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/License.txt\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/Sentences_75Agree.txt\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/Sentences_50Agree.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q -U \"transformers==4.36.2\" \"datasets==2.16.1\" \"accelerate==0.26.1\" \"bitsandbytes==0.42.0\"","metadata":{"execution":{"iopub.status.busy":"2024-07-04T05:18:58.308982Z","iopub.execute_input":"2024-07-04T05:18:58.309845Z","iopub.status.idle":"2024-07-04T05:19:31.019255Z","shell.execute_reply.started":"2024-07-04T05:18:58.309814Z","shell.execute_reply":"2024-07-04T05:19:31.017962Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngcsfs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2023.10.0 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\ns3fs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q -U git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e\n!pip install -q -U git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f","metadata":{"execution":{"iopub.status.busy":"2024-07-04T05:19:31.021579Z","iopub.execute_input":"2024-07-04T05:19:31.022412Z","iopub.status.idle":"2024-07-04T05:20:31.552183Z","shell.execute_reply.started":"2024-07-04T05:19:31.022373Z","shell.execute_reply":"2024-07-04T05:20:31.550922Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:13.122639Z","iopub.execute_input":"2024-07-04T06:54:13.123304Z","iopub.status.idle":"2024-07-04T06:54:13.127689Z","shell.execute_reply.started":"2024-07-04T06:54:13.123272Z","shell.execute_reply":"2024-07-04T06:54:13.126783Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer\nfrom trl import setup_chat_format\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig, \n                          TrainingArguments, \n                          pipeline, \n                          logging)\nfrom sklearn.metrics import (accuracy_score, \n                             classification_report, \n                             confusion_matrix)\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:14.234117Z","iopub.execute_input":"2024-07-04T06:54:14.234471Z","iopub.status.idle":"2024-07-04T06:54:14.241248Z","shell.execute_reply.started":"2024-07-04T06:54:14.234443Z","shell.execute_reply":"2024-07-04T06:54:14.240267Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"print(f\"pytorch version {torch.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:15.491630Z","iopub.execute_input":"2024-07-04T06:54:15.491997Z","iopub.status.idle":"2024-07-04T06:54:15.496710Z","shell.execute_reply.started":"2024-07-04T06:54:15.491967Z","shell.execute_reply":"2024-07-04T06:54:15.495841Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"pytorch version 2.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"working on {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:16.317216Z","iopub.execute_input":"2024-07-04T06:54:16.317642Z","iopub.status.idle":"2024-07-04T06:54:16.323350Z","shell.execute_reply.started":"2024-07-04T06:54:16.317588Z","shell.execute_reply":"2024-07-04T06:54:16.322380Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"working on cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sentiment-analysis-for-financial-news/all-data.csv',\n                names=[\"sentiment\", \"text\"],\n                 encoding=\"utf-8\", encoding_errors=\"replace\")","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:16.689977Z","iopub.execute_input":"2024-07-04T06:54:16.690779Z","iopub.status.idle":"2024-07-04T06:54:16.710277Z","shell.execute_reply.started":"2024-07-04T06:54:16.690745Z","shell.execute_reply":"2024-07-04T06:54:16.709520Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:17.113474Z","iopub.execute_input":"2024-07-04T06:54:17.113966Z","iopub.status.idle":"2024-07-04T06:54:17.124051Z","shell.execute_reply.started":"2024-07-04T06:54:17.113936Z","shell.execute_reply":"2024-07-04T06:54:17.123032Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"  sentiment                                               text\n0   neutral  According to Gran , the company has no plans t...\n1   neutral  Technopolis plans to develop in stages an area...\n2  negative  The international electronic industry company ...\n3  positive  With the new production plant the company woul...\n4  positive  According to the company 's updated strategy f...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n      <td>According to Gran , the company has no plans t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>neutral</td>\n      <td>Technopolis plans to develop in stages an area...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>The international electronic industry company ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>positive</td>\n      <td>With the new production plant the company woul...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>According to the company 's updated strategy f...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_train = []\nX_test = []\nfor sentiment in [\"positive\", \"neutral\", \"negative\"]:\n    train, test  = train_test_split(df[df.sentiment==sentiment], \n                                    train_size=300,\n                                    test_size=300, \n                                    random_state=42)\n    X_train.append(train)\n    X_test.append(test)\nX_train = pd.concat(X_train).sample(frac=1, random_state=10)\nX_test = pd.concat(X_test)\neval_idx = [idx for idx in df.index if idx not in list(X_train.index) + list(X_test.index)]\n# if it dont exist \nX_eval = df[df.index.isin(eval_idx)]\nX_eval = (X_eval\n          .groupby('sentiment', group_keys=False)\n          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\nX_train = X_train.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:17.444977Z","iopub.execute_input":"2024-07-04T06:54:17.445324Z","iopub.status.idle":"2024-07-04T06:54:18.774281Z","shell.execute_reply.started":"2024-07-04T06:54:17.445298Z","shell.execute_reply":"2024-07-04T06:54:18.773299Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:18.775967Z","iopub.execute_input":"2024-07-04T06:54:18.776256Z","iopub.status.idle":"2024-07-04T06:54:18.787023Z","shell.execute_reply.started":"2024-07-04T06:54:18.776231Z","shell.execute_reply":"2024-07-04T06:54:18.786039Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"    sentiment                                               text\n0     neutral  Mr Jortikka is president of the base metal div...\n1    positive  Both operating profit and net sales for the 12...\n2    negative  Finnish automation solutions developer Cencorp...\n3    positive  Renzo Piano 's building design will be a wonde...\n4    positive  `` We are proud to contribute to the creation ...\n..        ...                                                ...\n895   neutral  The dividend will be paid on April 15 , 2008 t...\n896   neutral  The new shares entitle their holders to divide...\n897   neutral  Activities range from the development of natur...\n898  positive  According to Bosse , the present cooperation i...\n899  positive  Operating profit rose to 22.1 mln eur from 19....\n\n[900 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n      <td>Mr Jortikka is president of the base metal div...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>positive</td>\n      <td>Both operating profit and net sales for the 12...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>Finnish automation solutions developer Cencorp...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>positive</td>\n      <td>Renzo Piano 's building design will be a wonde...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>`` We are proud to contribute to the creation ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>895</th>\n      <td>neutral</td>\n      <td>The dividend will be paid on April 15 , 2008 t...</td>\n    </tr>\n    <tr>\n      <th>896</th>\n      <td>neutral</td>\n      <td>The new shares entitle their holders to divide...</td>\n    </tr>\n    <tr>\n      <th>897</th>\n      <td>neutral</td>\n      <td>Activities range from the development of natur...</td>\n    </tr>\n    <tr>\n      <th>898</th>\n      <td>positive</td>\n      <td>According to Bosse , the present cooperation i...</td>\n    </tr>\n    <tr>\n      <th>899</th>\n      <td>positive</td>\n      <td>Operating profit rose to 22.1 mln eur from 19....</td>\n    </tr>\n  </tbody>\n</table>\n<p>900 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def generate_prompt(data_point):\n    return f\"\"\"\n            Analyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n\n            [{data_point[\"text\"]}] = {data_point[\"sentiment\"]}\n            \"\"\".strip()\n\n\ndef generate_test_prompt(data_point):\n    return f\"\"\"\n            Analyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n\n            [{data_point[\"text\"]}] = \"\"\".strip()\n\nX_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), \n                       columns=[\"text\"])\nX_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), \n                      columns=[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:18.788386Z","iopub.execute_input":"2024-07-04T06:54:18.788763Z","iopub.status.idle":"2024-07-04T06:54:18.812895Z","shell.execute_reply.started":"2024-07-04T06:54:18.788701Z","shell.execute_reply":"2024-07-04T06:54:18.812018Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"y_true = X_test.sentiment\nX_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n\ntrain_data = Dataset.from_pandas(X_train)\neval_data = Dataset.from_pandas(X_eval)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:18.814533Z","iopub.execute_input":"2024-07-04T06:54:18.814830Z","iopub.status.idle":"2024-07-04T06:54:18.843967Z","shell.execute_reply.started":"2024-07-04T06:54:18.814804Z","shell.execute_reply":"2024-07-04T06:54:18.843120Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def evaluate(y_true, y_pred):\n    labels = ['positive', 'neutral', 'negative']\n    mapping = {'positive': 2, 'neutral': 1, 'none':1, 'negative': 0}\n    def map_func(x):\n        return mapping.get(x, 1)\n    \n    y_true = np.vectorize(map_func)(y_true)\n    y_pred = np.vectorize(map_func)(y_pred)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n    print(f'Accuracy: {accuracy:.3f}')\n    \n    # Generate accuracy report\n    unique_labels = set(y_true)  # Get unique labels\n    \n    for label in unique_labels:\n        label_indices = [i for i in range(len(y_true)) \n                         if y_true[i] == label]\n        label_y_true = [y_true[i] for i in label_indices]\n        label_y_pred = [y_pred[i] for i in label_indices]\n        accuracy = accuracy_score(label_y_true, label_y_pred)\n        print(f'Accuracy for label {label}: {accuracy:.3f}')\n        \n    # Generate classification report\n    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n    print('\\nClassification Report:')\n    print(class_report)\n    \n    # Generate confusion matrix\n    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n    print('\\nConfusion Matrix:')\n    print(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:19.197480Z","iopub.execute_input":"2024-07-04T06:54:19.197913Z","iopub.status.idle":"2024-07-04T06:54:19.208272Z","shell.execute_reply.started":"2024-07-04T06:54:19.197879Z","shell.execute_reply":"2024-07-04T06:54:19.207338Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"model_name = \"ahxt/LiteLlama-460M-1T\"\n#model_name = \"NousResearch/Llama-2-7b-hf\"\ncompute_dtype = getattr(torch, \"float16\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, \n    bnb_4bit_quant_type=\"nf4\", \n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=device,\n    torch_dtype=compute_dtype,\n    quantization_config=bnb_config, \n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, \n                                          trust_remote_code=True,\n                                         )\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:19.618039Z","iopub.execute_input":"2024-07-04T06:54:19.618464Z","iopub.status.idle":"2024-07-04T06:54:22.267275Z","shell.execute_reply.started":"2024-07-04T06:54:19.618431Z","shell.execute_reply":"2024-07-04T06:54:22.266387Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"for i,v in X_train[:3].iterrows():\n    print(v['text'])\n    print('xxxxxxxx')","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:22.269008Z","iopub.execute_input":"2024-07-04T06:54:22.269330Z","iopub.status.idle":"2024-07-04T06:54:22.275467Z","shell.execute_reply.started":"2024-07-04T06:54:22.269302Z","shell.execute_reply":"2024-07-04T06:54:22.274603Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Analyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n\n            [Mr Jortikka is president of the base metal division of Outotec Oyj in Finland and is on the executive committee of Outotec .] = neutral\nxxxxxxxx\nAnalyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n\n            [Both operating profit and net sales for the 12-month period increased , respectively from EUR21 .5 m and EUR196 .1 m , as compared to 2005 .] = positive\nxxxxxxxx\nAnalyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n\n            [Finnish automation solutions developer Cencorp Corporation ( OMX Helsinki : CNC1V ) issued on Thursday ( 18 September ) a profit warning for the third quarter of 2008 .] = negative\nxxxxxxxx\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = '[Q: What is the largest bird?\\nA:]'\ntokenizer(prompt, return_tensors=\"pt\").input_ids","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:22.277048Z","iopub.execute_input":"2024-07-04T06:54:22.277466Z","iopub.status.idle":"2024-07-04T06:54:22.286868Z","shell.execute_reply.started":"2024-07-04T06:54:22.277431Z","shell.execute_reply":"2024-07-04T06:54:22.285838Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"tensor([[   58,    48,    25,  1867,   318,   262,  4387,  6512,    30,   198,\n            32, 47715]])"},"metadata":{}}]},{"cell_type":"code","source":"for i,v in X_train[:3].iterrows():\n    print(tokenizer.tokenize(v['text']))\n    print('xxxxxxxx')","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:22.408118Z","iopub.execute_input":"2024-07-04T06:54:22.409329Z","iopub.status.idle":"2024-07-04T06:54:22.420346Z","shell.execute_reply.started":"2024-07-04T06:54:22.409280Z","shell.execute_reply":"2024-07-04T06:54:22.418217Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"['Analy', 'ze', 'Ġthe', 'Ġsentiment', 'Ġof', 'Ġthe', 'Ġnews', 'Ġheadline', 'Ġenclosed', 'Ġin', 'Ġsquare', 'Ġbrackets', ',', 'Ġ', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġdetermine', 'Ġif', 'Ġit', 'Ġis', 'Ġpositive', ',', 'Ġneutral', ',', 'Ġor', 'Ġnegative', ',', 'Ġand', 'Ġreturn', 'Ġthe', 'Ġanswer', 'Ġas', 'Ġ', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġthe', 'Ġcorresponding', 'Ġsentiment', 'Ġlabel', 'Ġ', '\"', 'positive', '\"', 'Ġor', 'Ġ', '\"', 'neutral', '\"', 'Ġor', 'Ġ', '\"', 'negative', '\"', '.', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ[', 'Mr', 'ĠJ', 'ort', 'ik', 'ka', 'Ġis', 'Ġpresident', 'Ġof', 'Ġthe', 'Ġbase', 'Ġmetal', 'Ġdivision', 'Ġof', 'ĠOut', 'ot', 'ec', 'ĠOy', 'j', 'Ġin', 'ĠFinland', 'Ġand', 'Ġis', 'Ġon', 'Ġthe', 'Ġexecutive', 'Ġcommittee', 'Ġof', 'ĠOut', 'ot', 'ec', 'Ġ.', ']', 'Ġ=', 'Ġneutral']\nxxxxxxxx\n['Analy', 'ze', 'Ġthe', 'Ġsentiment', 'Ġof', 'Ġthe', 'Ġnews', 'Ġheadline', 'Ġenclosed', 'Ġin', 'Ġsquare', 'Ġbrackets', ',', 'Ġ', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġdetermine', 'Ġif', 'Ġit', 'Ġis', 'Ġpositive', ',', 'Ġneutral', ',', 'Ġor', 'Ġnegative', ',', 'Ġand', 'Ġreturn', 'Ġthe', 'Ġanswer', 'Ġas', 'Ġ', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġthe', 'Ġcorresponding', 'Ġsentiment', 'Ġlabel', 'Ġ', '\"', 'positive', '\"', 'Ġor', 'Ġ', '\"', 'neutral', '\"', 'Ġor', 'Ġ', '\"', 'negative', '\"', '.', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ[', 'Both', 'Ġoperating', 'Ġprofit', 'Ġand', 'Ġnet', 'Ġsales', 'Ġfor', 'Ġthe', 'Ġ12', '-', 'month', 'Ġperiod', 'Ġincreased', 'Ġ,', 'Ġrespectively', 'Ġfrom', 'ĠEUR', '21', 'Ġ.', '5', 'Ġm', 'Ġand', 'ĠEUR', '196', 'Ġ.', '1', 'Ġm', 'Ġ,', 'Ġas', 'Ġcompared', 'Ġto', 'Ġ2005', 'Ġ.', ']', 'Ġ=', 'Ġpositive']\nxxxxxxxx\n['Analy', 'ze', 'Ġthe', 'Ġsentiment', 'Ġof', 'Ġthe', 'Ġnews', 'Ġheadline', 'Ġenclosed', 'Ġin', 'Ġsquare', 'Ġbrackets', ',', 'Ġ', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġdetermine', 'Ġif', 'Ġit', 'Ġis', 'Ġpositive', ',', 'Ġneutral', ',', 'Ġor', 'Ġnegative', ',', 'Ġand', 'Ġreturn', 'Ġthe', 'Ġanswer', 'Ġas', 'Ġ', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġthe', 'Ġcorresponding', 'Ġsentiment', 'Ġlabel', 'Ġ', '\"', 'positive', '\"', 'Ġor', 'Ġ', '\"', 'neutral', '\"', 'Ġor', 'Ġ', '\"', 'negative', '\"', '.', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ[', 'F', 'inn', 'ish', 'Ġautomation', 'Ġsolutions', 'Ġdeveloper', 'ĠC', 'en', 'cor', 'p', 'ĠCorporation', 'Ġ(', 'ĠOM', 'X', 'ĠHelsinki', 'Ġ:', 'ĠC', 'NC', '1', 'V', 'Ġ)', 'Ġissued', 'Ġon', 'ĠThursday', 'Ġ(', 'Ġ18', 'ĠSeptember', 'Ġ)', 'Ġa', 'Ġprofit', 'Ġwarning', 'Ġfor', 'Ġthe', 'Ġthird', 'Ġquarter', 'Ġof', 'Ġ2008', 'Ġ.', ']', 'Ġ=', 'Ġnegative']\nxxxxxxxx\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"PAD token ID:\", tokenizer.pad_token_id)  # Should show the ID of the padding token\nprint(\"EOS token ID:\", tokenizer.eos_token_id)  # Should show the ID of the EOS token\nprint(\"PAD token:\", tokenizer.pad_token)        # Should show the actual padding token string\nprint(\"EOS token:\", tokenizer.eos_token) ","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:23.952594Z","iopub.execute_input":"2024-07-04T06:54:23.952964Z","iopub.status.idle":"2024-07-04T06:54:23.958781Z","shell.execute_reply.started":"2024-07-04T06:54:23.952935Z","shell.execute_reply":"2024-07-04T06:54:23.957678Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"PAD token ID: 50257\nEOS token ID: 50257\nPAD token: <|im_end|>\nEOS token: <|im_end|>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:26.384617Z","iopub.execute_input":"2024-07-04T06:54:26.385534Z","iopub.status.idle":"2024-07-04T06:54:26.394309Z","shell.execute_reply.started":"2024-07-04T06:54:26.385499Z","shell.execute_reply":"2024-07-04T06:54:26.393282Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(50259, 1024)\n    (layers): ModuleList(\n      (0-23): 24 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n          (k_proj): Linear4bit(in_features=1024, out_features=128, bias=False)\n          (v_proj): Linear4bit(in_features=1024, out_features=128, bias=False)\n          (o_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=1024, out_features=4096, bias=False)\n          (up_proj): Linear4bit(in_features=1024, out_features=4096, bias=False)\n          (down_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=1024, out_features=50259, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def predict(test, model, tokenizer):\n    y_pred = []\n    for i in tqdm(range(len(X_test))):\n        prompt = X_test.iloc[i][\"text\"]\n        pipe = pipeline(task=\"text-generation\", \n                        model=model, \n                        tokenizer=tokenizer, \n                        max_new_tokens = 1, \n                        temperature = 0.0,\n                       )\n        result = pipe(prompt)\n        answer = result[0]['generated_text'].split(\"=\")[-1]\n        if \"positive\" in answer:\n            y_pred.append(\"positive\")\n        elif \"negative\" in answer:\n            y_pred.append(\"negative\")\n        elif \"neutral\" in answer:\n            y_pred.append(\"neutral\")\n        else:\n            y_pred.append(\"none\")\n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:27.836468Z","iopub.execute_input":"2024-07-04T06:54:27.837322Z","iopub.status.idle":"2024-07-04T06:54:27.844389Z","shell.execute_reply.started":"2024-07-04T06:54:27.837289Z","shell.execute_reply":"2024-07-04T06:54:27.843458Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"y_pred = predict(test, model, tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:54:29.111522Z","iopub.execute_input":"2024-07-04T06:54:29.111883Z","iopub.status.idle":"2024-07-04T06:55:38.106569Z","shell.execute_reply.started":"2024-07-04T06:54:29.111854Z","shell.execute_reply":"2024-07-04T06:55:38.105583Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"100%|██████████| 900/900 [01:08<00:00, 13.05it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"evaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:55:38.108080Z","iopub.execute_input":"2024-07-04T06:55:38.108370Z","iopub.status.idle":"2024-07-04T06:55:38.130470Z","shell.execute_reply.started":"2024-07-04T06:55:38.108345Z","shell.execute_reply":"2024-07-04T06:55:38.129586Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Accuracy: 0.333\nAccuracy for label 0: 0.000\nAccuracy for label 1: 1.000\nAccuracy for label 2: 0.000\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       300\n           1       0.33      1.00      0.50       300\n           2       0.00      0.00      0.00       300\n\n    accuracy                           0.33       900\n   macro avg       0.11      0.33      0.17       900\nweighted avg       0.11      0.33      0.17       900\n\n\nConfusion Matrix:\n[[  0 300   0]\n [  0 300   0]\n [  0 300   0]]\n","output_type":"stream"}]},{"cell_type":"code","source":"output_dir=\"trained_weigths\"\n\npeft_config = LoraConfig(\n        lora_alpha=16, \n        lora_dropout=0.1,\n        r=64,\n        bias=\"none\",\n        target_modules=\"all-linear\",\n        task_type=\"CAUSAL_LM\",\n)\n\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,                    # directory to save and repository id\n    num_train_epochs=3,                       # number of training epochs\n    per_device_train_batch_size=1,            # batch size per device during training\n    gradient_accumulation_steps=8,            # number of steps before performing a backward/update pass\n    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n    optim=\"paged_adamw_32bit\",\n    save_steps=0,\n    logging_steps=25,                         # log every 10 steps\n    learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n    max_steps=-1,\n    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n    report_to=\"tensorboard\",                  # report metrics to tensorboard\n    evaluation_strategy=\"epoch\"               # save checkpoint every epoch\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:55:38.131452Z","iopub.execute_input":"2024-07-04T06:55:38.131745Z","iopub.status.idle":"2024-07-04T06:55:38.139527Z","shell.execute_reply.started":"2024-07-04T06:55:38.131701Z","shell.execute_reply":"2024-07-04T06:55:38.138666Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=train_data,\n    eval_dataset=eval_data,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    max_seq_length=1024,\n    packing=False,\n    dataset_kwargs={\n        \"add_special_tokens\": False,\n        \"append_concat_token\": False,\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:55:38.141570Z","iopub.execute_input":"2024-07-04T06:55:38.142014Z","iopub.status.idle":"2024-07-04T06:55:39.229526Z","shell.execute_reply.started":"2024-07-04T06:55:38.141981Z","shell.execute_reply":"2024-07-04T06:55:39.228685Z"},"trusted":true},"execution_count":57,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e6429327e784d379c13db74ed14107e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"728bf495f4e2413aaf2030fef6606770"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-04T06:55:39.230693Z","iopub.execute_input":"2024-07-04T06:55:39.231064Z","iopub.status.idle":"2024-07-04T07:14:54.967918Z","shell.execute_reply.started":"2024-07-04T06:55:39.231031Z","shell.execute_reply":"2024-07-04T07:14:54.966954Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stderr","text":"You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [336/336 19:11, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.927700</td>\n      <td>0.795103</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.752400</td>\n      <td>0.750556</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=336, training_loss=0.9340126131262098, metrics={'train_runtime': 1155.1251, 'train_samples_per_second': 2.337, 'train_steps_per_second': 0.291, 'total_flos': 854203480940544.0, 'train_loss': 0.9340126131262098, 'epoch': 2.99})"},"metadata":{}}]},{"cell_type":"code","source":"# Save trained model and tokenizer\n# trainer.save_model()\n# tokenizer.save_pretrained(output_dir)\nnew_model = \"LiteLlama-460M-1T-sentiment-ana\"\n\ntrainer.model.save_pretrained(new_model)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T07:18:55.233230Z","iopub.execute_input":"2024-07-04T07:18:55.233984Z","iopub.status.idle":"2024-07-04T07:18:56.585999Z","shell.execute_reply.started":"2024-07-04T07:18:55.233949Z","shell.execute_reply":"2024-07-04T07:18:56.585032Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# %load_ext tensorboard\n# %tensorboard --logdir logs/runs\nnew_model = \"LiteLlama-460M-1T-sentiment-ana\"\nbase_model = \"ahxt/LiteLlama-460M-1T\"","metadata":{"execution":{"iopub.status.busy":"2024-07-04T07:19:21.408068Z","iopub.execute_input":"2024-07-04T07:19:21.408454Z","iopub.status.idle":"2024-07-04T07:19:21.413244Z","shell.execute_reply.started":"2024-07-04T07:19:21.408422Z","shell.execute_reply":"2024-07-04T07:19:21.412195Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel","metadata":{"execution":{"iopub.status.busy":"2024-07-04T07:22:29.583533Z","iopub.execute_input":"2024-07-04T07:22:29.584259Z","iopub.status.idle":"2024-07-04T07:22:29.588668Z","shell.execute_reply.started":"2024-07-04T07:22:29.584229Z","shell.execute_reply":"2024-07-04T07:22:29.587677Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"#trainer.model.save_pretrained(new_model)\nfrom peft import AutoPeftModelForCausalLM\n# Reload model in FP16 and merge it with LoRA weights\n# model = AutoModelForCausalLM.from_pretrained(\n#     base_model,\n#     torch_dtype=compute_dtype,\n#      return_dict=False,\n#      low_cpu_mem_usage=True,\n#      device_map=device,\n# )\n# model = PeftModel.from_pretrained(model, new_model)\n# model = model.merge_and_unload()\n\n# # Reload tokenizer to save it\n# tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = \"right\"\n\n\n\n\n\n# from peft import AutoPeftModelForCausalLM\n\nfinetuned_model = \"./trained_weigths/\"\ncompute_dtype = getattr(torch, \"float16\")\ntokenizer = AutoTokenizer.from_pretrained(\"ahxt/LiteLlama-460M-1T\")\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n     finetuned_model,\n     torch_dtype=compute_dtype,\n     return_dict=False,\n     low_cpu_mem_usage=True,\n     device_map=device,\n)\n\nmerged_model = model.merge_and_unload()\nmerged_model.save_pretrained(\"./merged_model\",safe_serialization=True, max_shard_size=\"2GB\")\ntokenizer.save_pretrained(\"./merged_model\")","metadata":{"execution":{"iopub.status.busy":"2024-07-04T07:24:37.940045Z","iopub.execute_input":"2024-07-04T07:24:37.940879Z","iopub.status.idle":"2024-07-04T07:24:42.840039Z","shell.execute_reply.started":"2024-07-04T07:24:37.940845Z","shell.execute_reply":"2024-07-04T07:24:42.839097Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"('./merged_model/tokenizer_config.json',\n './merged_model/special_tokens_map.json',\n './merged_model/vocab.json',\n './merged_model/merges.txt',\n './merged_model/added_tokens.json',\n './merged_model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"# import gc\n\n# del [model, tokenizer, peft_config, trainer, train_data, eval_data, bnb_config, training_arguments]\n# del [df, X_train, X_eval]\n# del [TrainingArguments, SFTTrainer, LoraConfig, BitsAndBytesConfig]\n# for _ in range(100):\n#     torch.cuda.empty_cache()\n#     gc.collect()\ny_pred = predict(test, merged_model, tokenizer)\nevaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T07:25:03.979030Z","iopub.execute_input":"2024-07-04T07:25:03.979740Z","iopub.status.idle":"2024-07-04T07:25:36.243576Z","shell.execute_reply.started":"2024-07-04T07:25:03.979706Z","shell.execute_reply":"2024-07-04T07:25:36.242648Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stderr","text":"100%|██████████| 900/900 [00:32<00:00, 27.91it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.767\nAccuracy for label 0: 0.887\nAccuracy for label 1: 0.770\nAccuracy for label 2: 0.643\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.85      0.89      0.87       300\n           1       0.67      0.77      0.72       300\n           2       0.79      0.64      0.71       300\n\n    accuracy                           0.77       900\n   macro avg       0.77      0.77      0.77       900\nweighted avg       0.77      0.77      0.77       900\n\n\nConfusion Matrix:\n[[266  28   6]\n [ 24 231  45]\n [ 22  85 193]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"### bump in accuracy from 0.33 to 0.77","metadata":{},"execution_count":null,"outputs":[]}]}